\documentclass[aspectratio=169, compress]{beamer}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{animate}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usepackage{bm}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}
\usepackage{amsmath,amssymb,amsfonts, stmaryrd}
\usepackage{pgfplots}
\usetikzlibrary{shapes,arrows,decorations.pathmorphing,backgrounds,positioning,fit,matrix}
\usepackage{mathtools} %Fixes/improves amsmath
\usepackage{lipsum}
\pgfplotsset{compat=1.8}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usetikzlibrary{spy}
% Latin Modern
\usepackage{lmodern}
% Verdana font type
%\usepackage{verdana}
% Helvetica
%\usepackage{helvet}
% Times (text and math)
%\usepackage{newtx, newtxmath}
% Nice font combination
%\usepackage{mathptmx} % math
%\usepackage{sourcesanspro} % sans-serif
\usepackage{charter} % serif

\usetheme[department=space]{DTU}
%\useoutertheme[subsection=false]{miniframes}
%\useoutertheme{split}
\setbeamercovered{transparent}
%\setcounter{currentslide}{\the\beamer@slideinframe}

\title{Photogrammetry (30540)}
\subtitle{Feature Detection, Matching \& RANSAC}
\author{\textbf{Instructor}: Daniel Olesen$^1$ Xiao Hu$^2$\\
	\textbf{TA}: Xiao Hu$^2$}
\institute{$^1$: Assistant Professor, Dept. of Geodesy\\ $2$: PhD student}
\date{\today}

\newcommand{\tabitem}{{\color{dtured}$\bullet$} }

\begin{document}
	\frame{
		\maketitle
	}
	
	\frame{
		\frametitle{Today's Lecture}
		\tableofcontents
	}
	
	
	
	\section{Image Feature}
	\subsection{Feature Detection}
	\frame{
		\frametitle{Feature Definition} 
		Features are recognizable structures of elements in the environment and serve as a more compact and robust description of the environment.
		\begin{itemize}
		\item low-level features (geometric primitives): lines, \textcolor{red}{points}, blobs, circles or polygons.
		\item high-level features (sematic objects): signs, etc.
		\end{itemize}
	}

	\frame{
		\frametitle{Low-level features: edges}
		 \begin{figure}
			\centering
			%\includegraphics[width=0.4\textwidth]{figures/edge1.png}
			\includegraphics[width=0.4\textwidth]{figures/cheesboard.png}
			\caption{Illustration from\cite{c17}.}
		\end{figure}	
	}	
	
	\frame{
		\frametitle{Low-level features: edges}
		 \begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{figures/edge.png}
		\end{figure}	
	}
	\frame{
		\frametitle{Low-level features: \textcolor{red}{points}}
		 \begin{figure}
			\centering
			\includegraphics[width=0.6\textwidth]{figures/xiaohan_harris.jpg}
		\end{figure}	
	}	
	\frame{
		\frametitle{Low-level features: circles}
		 \begin{figure}
			\centering
			\includegraphics[width=0.9\textwidth]{figures/conic.png}
			\caption{Conic detection from \cite{c16}}
		\end{figure}	
	}	
	\frame{
		\frametitle{High-level features: sign}
		 \begin{figure}
			\centering
			\includegraphics[width=0.7\textwidth]{figures/sign.png}
			\caption{Sign detection from \cite{c15}}
		\end{figure}	
	}	
	\frame{
		\frametitle{High-level features: road}
		 \begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{figures/road.png}
			\caption{Road detection from \cite{c14}}
		\end{figure}	
	}	

	\frame{
		\frametitle{Application of Point features: Visual odometry}
			\begin{figure}
				\centering
				\begin{minipage}{1\textwidth}
					\centering
			\animategraphics[loop,controls,width=0.65\linewidth]{10}{figures/vo/vo-}{0}{289}
				\end{minipage}%
			\end{figure}
	}	
	
	\frame{
		\frametitle{Application of Point features: Mosaicing}
			\begin{figure}
				\centering
				\begin{minipage}{1\textwidth}
					\centering
				\includegraphics[width=0.6\textwidth]{figures/pano1.png}
				\end{minipage}%
			\end{figure}
	}	
	
	\frame{
	\frametitle{Application of Point features: 2D Mapping with Aerial images}
	\begin{figure}
		\centering
		\begin{minipage}{1\textwidth}
			\centering				
			\animategraphics[loop,controls,width=0.65\linewidth]{10}{figures/map2dfusion/map2d-}{0}{260}
		\end{minipage}%
	\end{figure}
}		
	
	\frame{
		\frametitle{Feature Definition} 
		Features are recognizable structures of elements in the environment: 
		\begin{itemize}
		\item low-level features (geometric primitives): lines, \textcolor{red}{points}, blobs, circles or polygons.
		\item high-level features (sematic objects): doors, tables, or trash cans.
		\end{itemize}
		Features serve as more compact and robust description of the environment.

		\begin{figure}
			\centering
			%\includegraphics[width=0.6\textwidth]{figures/edge.jpg}
			\includegraphics[width=0.3\textwidth]{figures/xiaohan_harris.jpg}
		\end{figure}
	}	

	\frame{
		\frametitle{Feature Detection:  Keypoint Detectors}
		Properties of the ideal feature detector:
		\begin{itemize}
			\item Repeatability: can be redetected regardless view/illumination changes: \textcolor{red}{rotation, scale (zoom), and illumination invariant}.
			\item Distinctiveness: easy to distinguish and match.
			\item Localization accuracy.
			\item Computational efficiency.
		\end{itemize}
		Two major groups:
		\begin{enumerate}
			\item Corner detectors: Harris\cite{c2}, FAST\cite{c3}, etc.
			\item Blob feature detectors: SIFT\cite{c4}, etc.
		\end{enumerate}
	}
	\frame{
		\frametitle{Corner detector: Harris}
		A corner in an image can be defined as the intersection of two or more edges (lines).
		\begin{figure}
			\centering
			\includegraphics[width=0.45\textwidth]{figures/corner5.png}
			\caption{Illustration of Corner keypoint\cite{c18}.}
		\end{figure}
		}

	\frame{
		\frametitle{Corner detector: Harris}
		A corner in an image can be defined as the intersection of two or more edges (lines).
		\begin{figure}
			\centering
			\includegraphics[width=0.6\textwidth]{figures/corner0.png}
			\caption{Illustration of Corner keypoint\cite{c18}.}
		\end{figure}
		What should be inside the sliding window $\to$ the partial derivatives of the Sum of
Squared Differences (SSD).
		}
	\frame{
		\frametitle{Corner detector: Harris}
			SSD:
			$$\text{SSD}(x,y)=\sum_{u}\sum_{v}{(I(u,v)-I(u+x,v+y))^2}$$
			$I(u+x,v+y)$ can be approximated by a first-order Taylor expansion using partial derivatives $I_x$ and $I_y$:
			$$I(u+x,v+y)\approx I(u,v)+I_x(u,v)x+I_y(u,v)y$$
			This produces the approximation of SSD:
			$$
			\text{SSD}(x,y)\approx \sum_{u}\sum_{v} (I_x(u,v)x+I_y(u,v)y)^2 = [x,y]\underbrace{\left(\sum_{u}\sum_{v}\left[\begin{matrix}
			I_x^2 & I_xI_y \\
			 I_xI_y &  I_y^2
			\end{matrix}\right]\right)}_{\mathbf{M}}[x,y]^T
			$$
		}

	\frame{
		\frametitle{Corner detector: Harris}
			Now we know what's inside the sliding window:
		\begin{figure}
			\centering
			\includegraphics[width=0.8\textwidth]{figures/ssd.png}
			%\caption{.%}
		\end{figure}
		}		
		
	\frame{
		\frametitle{Corner detector: Harris}
			\begin{columns}
			\column[c]{.5\textwidth}		
			\begin{figure}
				\centering
				\includegraphics[width=1.1\textwidth]{figures/corner2.png}
				\caption{The Harris detector analyses the eigenvalues of $\mathbf{M}$ to verify whether currecnt point is a corner or not\cite{c6}.}	
%				The classification of corner and edges according to Harris and
%Stephens}
			\end{figure}
			\column[c]{.5\textwidth}
			For easy computation, the following “cornerness function” is actually used:
			$$
			C = \lambda_1\lambda_2-k(\lambda_1+\lambda_2)^2=\text{det}(\mathbf{M})-k\cdot \text{trace}(\mathbf{M})^2
			$$
			Harris corner point is then found as the local maximum. Nonmaxima suppression is used to remove multiple detections. 
			
			In conclusion, Harris keypoint is \textcolor{red}{rotation/translation invariant, but not scale invariant.}
			\end{columns}
		}
	\frame{
		\frametitle{Corner detector: Harris}
		Let's see an example:
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{figures/corner3.png}
		\end{figure}
	}
	\frame{
		\frametitle{Other corner detectors}
		\begin{itemize}
			\item Shi-Tomasi corner detector or GFTT: use $min(\lambda_1,\lambda_2)$.
			\item FAST: compares $16$ pixels on a circle around the candidate corner. Not rotation invariant, but super fast.
		\end{itemize}
		\begin{figure}
			\centering
			\includegraphics[width=0.8\textwidth]{figures/corner4.png}
			\caption{Comparison of Harris and FAST.}
		\end{figure}
	}
	
	
	\frame{
		\frametitle{Blob Detectors}
		Blob is an pattern which differs from its immediate neighborhood in terms of
		intensity, color, and texture. 
		
		\begin{figure}
		\centering
		\includegraphics[width=0.5\textwidth]{figures/ex26.png}
		\caption{Blobs.}
		\end{figure}		
	}
	\frame{		
		\frametitle{SIFT}
		Scale Invariant Feature Transform(SIFT)\cite{c4} is the most popular blob feature detector, which is widly used in mapping and navigation, etc thanks to its robustness to rotation and small changes of illumination, scale.
		
%		Main steps of the SIFT algorithm:
%		\begin{itemize}
%			\item find keypoint location and scale.
%			\item find dominant orientation.
%			\item generate keypoint descriptor.
%		\end{itemize}
%		So, SIFT $=$ keypoint $+$ descriptor.
	}

	\frame{
	\frametitle{Comparison: SIFT V.S. Harris+BRIEF}
	\begin{minipage}{0.5\textwidth}
	\begin{figure}
			\centering				
			\animategraphics[loop,controls,width=1\linewidth]{6}{figures/harrisbrief/harrisbrief-}{1}{36}
	\end{figure}
	\end{minipage}%
	\begin{minipage}{0.5\textwidth}
	\begin{figure}
		\centering				
		\animategraphics[loop,controls,width=1\linewidth]{6}{figures/sift/sift-}{1}{36}
	\end{figure}
	\end{minipage}%

}	


	\frame{		
		\frametitle{SIFT}
%		Scale Invariant Feature Transform(SIFT)\cite{c4} is the most popular blob feature detector, which is widly used in mapping and navigation, etc thanks to its robustness to rotation and small changes of illumination, scale.
		
		Main steps of the SIFT algorithm:
		\begin{itemize}
			\item find keypoint location and scale.
			\item find dominant orientation.
			\item generate keypoint descriptor.
		\end{itemize}
		So, SIFT $=$ keypoint $+$ descriptor.
	}

	\frame{
		\frametitle{Find keypoint location and scale}
			\begin{columns}
			\column[c]{.4\textwidth}
			\begin{itemize}
				\item Build up scale space.
				\item Compute Difference of Gaussian (DoG).
				\item Find local extrema across adjacent scales, then additional substeps like elimination points on edges or in very flat regions, interpolation of feature's location.
			\end{itemize}
			\column[c]{.6\textwidth}
			\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{figures/sift1.png}
			\caption{Scale space and DoG images.\cite{c6}.}
			\end{figure}
		\end{columns}
	}

	\frame{
	\frametitle{Find dominant orientation.}
	\begin{columns}
		\column[c]{.5\textwidth}
		The dominant orientation is used to make the descriptor rotation invariant.
		\begin{itemize}
			\item Compute the gradient magnitude and orientation in the image with closet scale.
			\item Build a gradient magnitude weighted histogram of orientations within a Gaussian-weighted circular window. 
			\item Find the peaks as the dominant orientation. If the second peak is within 80\% of the first peak, an additional keypoint is created.
		\end{itemize}
		\column[c]{.5\textwidth}
		\begin{figure}
			\centering
			\includegraphics[width=0.9\textwidth]{figures/sift2.png}
			\caption{Histogram of orientation and examples of SIFT features with different scales and dominant orientations\cite{c6}.}
		\end{figure}
	\end{columns}
	}

	\frame{
	\frametitle{Generate feature descriptor}
	\begin{columns}
	\column[c]{.5\textwidth}
	\begin{itemize}
		\item Rotate the gradient orientations relative to the keypoint dominant orientation.
		\item The Gaussian window  is then divided into $4\times 4$ regions. A second histogram of gradient with eight orientation bins is computed within each region.
		\item The descriptor is built by flattening the orientation histograms and concatenating them to form a $4\times 4 \times 8=128$ vector.
		\item Normalization, clipping and renormalization for illumination invariant.
	\end{itemize}
	
	\column[c]{.5\textwidth}
	\begin{figure}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/sift3.png}
		\caption{Gaussian circular windown and $4\times 4$ regions with histograms of gradient with $8$ bins. This figure shows a $2\times2$ descriptor array computed from an $8\times 8$ set of samples, whereas SIFT will use $4\times 4$ descriptors computed from a $16\times 16$ sample array\cite{c4}.}
	\end{figure}
\end{columns}
	}	

	\subsection{Feature Description}
	\frame{
	\frametitle{Why descriptor}
		Keypoints alone are difficult to match, only $(u,v)$ and scale. 
		
		Just like human being, we need the name, CPR-number, student-id, etc. to match the corresponding data or affairs belonging to you.
		
		Feature descriptor serves as the CPR-number for keypoint. For feature matching, we actually match their descriptors. 
	}
	\frame{
	\frametitle{Descriptor}
	\begin{columns}
		\column[c]{.5\textwidth}
		Descriptor is often computed from the neighbours of the keypoint. Normally the neighbouring information is utilized in the following ways:
		\begin{itemize}
			\item Histogram of Gradients (HoG) descriptors, like SIFT.
			\item Binary descriptors, like BRIEF\cite{c8}.
		\end{itemize}
			\column[c]{.5\textwidth}
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{figures/desp1.png} \\
			\includegraphics[scale=0.1]{figures/desp2.png}
			\caption{Histogram of Gradients (HoG) descriptor and Binary descriptor.}
		\end{figure}
		\end{columns}
	}		

	\subsection{Summary on Features}
	\frame{
		\frametitle{Summary}
		\begin{figure}
			\centering
			\includegraphics[width=0.6\textwidth]{figures/desp3.png}
			\caption{Comparison of feature detectors: properties and performance\cite{c6}.}
		\end{figure}
	}
	\frame{
		\frametitle{Summary}
		\begin{itemize}
			\item SIFT is the most used feature in modern Photogrammetry.
			\item ORB and Harris are often used in visual odometry, VSLAM, tracking.
			\item Hand-crafted features are now challenged by deep learning based features, e.g. LIFT\cite{c9} V.S. SIFT. More learning based features are emerging. However, their usages in Photogrammetry are still in development (could be an research interest).
		\end{itemize}
		\begin{figure}
			\centering
			\includegraphics[width=1\textwidth]{figures/lift2.png}
			\caption{Comparison of Superpoint, LIFT, SIFT, and ORB\cite{c10}.}
		\end{figure}
	}
	\section{Feature Matching}
	{
		\subsection{Principles}	
		\frame{
			\frametitle{Principles}
			\begin{columns}
				\column[c]{.5\textwidth}
				\begin{figure}
					\centering
					\includegraphics[width=1\textwidth]{figures/matching1.png}
					\caption{Principle of feature matching\cite{c11}.}
				\end{figure}
				\column[c]{.5\textwidth}
				Different distance functions for comparing the similarity of two descriptors:
				\begin{itemize}
					\item $\text{L}_2$ distance, for descriptors with floating value (SIFT):
					$$
					d(f_a,f_b) = \sum_i||(f_a(i)-f_b(i))||_2
					$$
					\item Hamming distance, for descriptors with binary value (BRIEF):
					$$
					d(f_a,f_b) = \sum_i\text{XOR}(f_a(i),f_b(i))
					$$
					\item others: $\text{L}_1$ distance, etc.
				\end{itemize}
			\end{columns}
		}	
	\subsection{Matching strategy}	
	\frame{
	\frametitle{Matching strategy}
		\begin{columns}
			\column[c]{.65\textwidth}
			Comparing strategy:
			\begin{itemize}
				\item Brute-force matching: for $a$ in image $1$, compare all $b$ in image $2$.
				\item Kd tree: binary search in Kd tree.
			\end{itemize}
			Selecting strategy:
			\begin{itemize}
				\item Take the nearest.
				\item Take the nearest with the distance below a threshold.
				\item Take the two closest and perform ratio test:
						$$
						\frac{d_{closet}}{d_{second\_closet}} < \tau
						$$
						$\tau$ is usually set to $0.7$.
				\item Cross-check: $f_b$ is the best match for $f_a$ in image $2$ and $f_a$ is the best match for $f_b$ in image $1$.
			\end{itemize}
			\column[c]{.35\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=0.9\textwidth]{figures/matching2.png}
				\caption{Feature matching using Kd tree\cite{c12}.}
			\end{figure}
		\end{columns}
	}
	\frame{
		\frametitle{Example}
		\begin{columns}
			\column[c]{.5\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=1.\textwidth]{figures/fast.png}
				\caption{FAST$+$BRIEF matching with ratio test.}
			\end{figure}
			\column[c]{.5\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=1.\textwidth]{figures/sift.png}
				\caption{SIFT matching with ratio test.}
			\end{figure}
		\end{columns}
	}
	}
	
	\section{RANSAC}
	\subsection{Principle}
	\frame{
		\frametitle{Principle}
		\begin{columns}
			\column[c]{.5\textwidth}
			\textbf{Why need RANSAC?}
			
			Feature matching result often contains a certain amount of false matches (\textbf{Outliers}). Outlier will severally deteriorate the estimation result.
						
			Generally speaking, \textbf{RANdom SAmple Consensus (RANSAC)\cite{c13} is an iterative method for estimating model parameters from observations containing outliers}.

			The model could be a line, a parabola, an ellipse, etc.			
			
			\column[c]{.5\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=1.\textwidth]{figures/ransac1.png}
				\caption{Line estimation results.}
			\end{figure}
		\end{columns}			
	}
	\frame{
		\frametitle{RANSAC Pipeline}
		\begin{columns}
			\column[c]{.5\textwidth}
			\begin{itemize}
				\item Randomly select $n$ samples from observations, $n$ is the minimum number of samples needed for model estimation, e.g. $n=2$ for 2D line, $n=3$ for 3D plane, etc.						\item Model estimation, could be LS, SVD, etc.
				\item Compute Consensus: apply a model-specific loss function to each observation and the model obtained, the response could serve as the consensus, e.g. point-line distance, point-plane distance.
				\item Classifier inliers and outliers using a predefined threshold and log the model with the maximum number of inliers.
				\item Iterate.
			\end{itemize}
			\column[c]{.5\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=0.7\textwidth]{figures/ransac7.png}
				\caption{RANSAC Procedure.}
			\end{figure}
		\end{columns}			
	}
	\frame{
		\frametitle{How many iterations to choose}
		Let $p_{out}$ be the probability that one point is an outlier, $n$ be the minimum number of samples for model estimation, $N$ be the number of iterations, $p$ be the desired probability that we get a good sample:
		$$
		p=1-({1-(1-p_{out})^n})^N
		$$
		\begin{itemize}
		\item $1-p_{out}$: Probability of an inlier.
		\item $(1-p_{out})^n$: Probability of choosing $n$ inliers.
		\item $({1-(1-p_{out})^n})$: Probability of at least one items in the sample being outliers for one iteration.
		\item $({1-(1-p_{out})^n})^N$: Probability that $N$ iterations are contaminated.
		\item $1-({1-(1-p_{out})^n})^N$: Probability that at least one iteration is not
contaminated.
		\end{itemize}
		Usually, we set $p$ and then compute $N$ backwardly as $N = \frac{\log(1-p)}{\log(1-(1-p_{out})^n)}$.
	}
	\frame{
		\frametitle{How many iterations to choose}
		A look-up table for $N$ when $p$ is set to $0.99$.
		\begin{table}
    	\begin{tabular}{|llllllll|}
        \hline
        ~ & ~ & ~ & ~ & $p_{out}$ & ~ & ~ & ~ \\ \hline
        n & $5\%$ & $10\%$ & $20\%$ & $25\%$ & $30\%$ & $40\%$ & $50\%$ \\ 
        2 & 2 & 3 & 5 & 6 & 7 & 11 & 17 \\ 
        3 & 3 & 4 & 7 & 9 & 11 & 19 & 35 \\ 
        4 & 3 & 5 & 9 & 13 & 17 & 34 & 72 \\ 
        5 & 4 & 6 & 12 & 17 & 26 & 57 & 146 \\ 
        6 & 4 & 7 & 16 & 24 & 37 & 97 & 293 \\ 
        7 & 4 & 8 & 20 & 33 & 54 & 163 & 588 \\ 
        8 & 5 & 9 & 26 & 44 & 78 & 272 & 1177 \\
        \hline
    	\end{tabular}
		\end{table}
	}
	\frame{
		\frametitle{Example of RANSAC}
		\begin{figure}
			\centering
			\includegraphics[width=0.8\textwidth]{figures/ransac8.png}
			\caption{Feature matching before applying RANSAC.}
		\end{figure}		
		As you can see, false matches get hypothesized.Let's apply RANSAC $+$ epipolar constraint.
	}
	\frame{
		\frametitle{Example of RANSAC}
		\begin{figure}
			\centering
			\includegraphics[width=0.8\textwidth]{figures/ransac9.png}
			\caption{Feature matching after applying RANSAC.}
		\end{figure}		
		As you can see, false matches are removed.
	}

%	\begin{minipage}{\textwidth}
%			\begin{minipage}{.325\textwidth}
%				\centering
%
%			\end{minipage}
%			\begin{minipage}{.325\textwidth}
%				\centering
%
%			\end{minipage}
%			\begin{minipage}{.325\textwidth}
%				\centering
%
%			\end{minipage}
%	\end{minipage}
%	}

	\section{Reference}
	\subsection{Ending}
	\frame{
		\centering
		{\textbf{Thank You!}
		}
	}
	
	\begin{thebibliography}{00}
%		\bibitem{c3} S. {Hadfield} and K. {Lebeda} and R. {Bowden}, ``HARD-PnP: PnP Optimization Using a Hybrid Approximate Representation,`` IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41. pp.  768-774, 2019.
	\bibitem{c1} \url{https://en.wikipedia.org/wiki/Mars_Exploration_Rover}.
	\bibitem{c2}  Chris Harris and Mike Stephens (1988). "A Combined Corner and Edge Detector". Alvey Vision Conference. 15.
	\bibitem{c3} Rosten, Edward, and Tom Drummond. "Machine learning for high-speed corner detection." European conference on computer vision. Springer, Berlin, Heidelberg, 2006.
	\bibitem{c4} Lowe, David G. "Distinctive image features from scale-invariant keypoints." International journal of computer vision 60.2 (2004): 91-110.
	\bibitem{c5} \url{https://www.uio.no/studier/emner/matnat/its/UNIK4690/v16/forelesninger/lecture_3_2_1_corner_features.pdf}.
	\bibitem{c6} Siegwart, Roland, Illah Reza Nourbakhsh, and Davide Scaramuzza. Introduction to autonomous mobile robots. MIT press, 2011.
	\bibitem{c7} J. Shi and C. Tomasi. Good Features to Track,. 9th IEEE Conference on Computer Vision and Pattern Recognition. Springer. June 1994. 
	\bibitem{c8} Calonder, Michael, et al. "BRIEF: Computing a local binary descriptor very fast." IEEE Transactions on Pattern Analysis and Machine Intelligence 34.7 (2012): 1281-1298.
	\bibitem{c9} Yi, Kwang Moo, et al. "Lift: Learned invariant feature transform." European Conference on Computer Vision. Springer, Cham, 2016.
	\bibitem{c10} DeTone, Daniel, Tomasz Malisiewicz, and Andrew Rabinovich. "Superpoint: Self-supervised interest point detection and description." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2018.
	\bibitem{c11} \url{ https://www.uio.no/studier/emner/matnat/its/UNIK4690/v16/forelesninger/lecture_4_2_feature_matching.pdf}
	\bibitem{c12} \url{https://www.cs.ubc.ca/~nando/nipsfast/slides/fast04.pdf}
	\bibitem{c13} Fischler, Martin A., and Robert C. Bolles. "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography." Communications of the ACM 24.6 (1981): 381-395.
	\bibitem{c14} Mendes, Caio César Teodoro, Vincent Frémont, and Denis Fernando Wolf. "Exploiting fully convolutional neural networks for fast road detection." 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016.
	\bibitem{c15} Wang, Bihao, et al. "Landmarks based human-like guidance for driving navigation in an urban environment." 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2017.
	\bibitem{c16} Hua, Minh-Duc, et al. "Explicit complementary observer design on Special Linear Group SL (3) for homography estimation using conic correspondences." 2017 IEEE 56th Annual Conference on Decision and Control (CDC). IEEE, 2017.
	\bibitem{c17} F\"orstner, Scriptum Photogrammetry I, Chapter “Kantenextraktion".
	\bibitem{c18} Robert Collins, CSE486, Penn State, Lecture 06: Harris Corner Detector.

	\end{thebibliography}	
	
\end{document}